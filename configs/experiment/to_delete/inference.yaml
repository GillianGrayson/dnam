# @package _global_

project_name: inference

seed: 322
in_dim: 13518
out_dim: 3

work_dir: "E:/YandexDisk/Work/pydnameth/datasets/meta/tasks/GPL13534_Blood_ICD10-V/R/one_by_one/classification/models/${project_name}"
data_dir: "E:/YandexDisk/Work/pydnameth/datasets/meta/tasks/GPL13534_Blood_ICD10-V/R/one_by_one/classification"

ckpt_path: "${data_dir}/models/tabnetpl/logs/runs/2022-02-13_22-22-52/checkpoints"
ckpt_name: "105.ckpt"
n_top_feat: 30
n_examples: 3
explainer_type: kernel

# specify here default training configuration
defaults:
  - override /datamodule: betas_pheno_datamodule.yaml
  - override /trainer: default.yaml # choose trainer from 'configs/trainer/'
  - override /model: tabnetpl.yaml
  - override /callbacks: regular.yaml
  - override /logger: many_loggers.yaml # set logger here or use command line (e.g. `python run.py logger=wandb`)
  - override /hydra/hydra_logging: colorlog
  - override /hydra/job_logging: colorlog

datamodule_train_val:
  _target_: src.datamodules.dnam.betas_pheno.DNAmDataModuleTogether
  path: ${data_dir}
  cpgs_fn: "${data_dir}/cpgs/${in_dim}.xlsx"
  statuses_fn: "${data_dir}/statuses/${out_dim}.xlsx"
  dnam_fn: "mvals_regRCPqn.pkl"
  pheno_fn: "pheno_regRCPqn.pkl"
  outcome: "Status"
  batch_size: 64
  train_val_test_split: [0.8, 0.2, 0.0]
  num_workers: 0
  pin_memory: False
  seed: ${seed}
  weighted_sampler: True

datamodule_test:
  _target_: src.datamodules.dnam.betas_pheno.DNAmPhenoInferenceDataModule
  path: "${data_dir}/data_test"
  cpgs_fn: "${data_dir}/cpgs/${in_dim}.xlsx"
  statuses_fn: "${data_dir}/statuses/${out_dim}.xlsx"
  dnam_fn: "mvals_GSE113725_regRCPqn.pkl"
  pheno_fn: "pheno_GSE113725.pkl"
  outcome: "Status"
  batch_size: 64
  num_workers: 0
  pin_memory: False
  imputation: "median"

trainer:
  gpus: 1
  min_epochs: 1
  max_epochs: 1000
  weights_summary: null
  progress_bar_refresh_rate: 10
  resume_from_checkpoint: null

model:
  task: "classification"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  n_d_n_a: 8
  n_steps: 3
  gamma: 1.3
  n_independent: 1
  n_shared: 2
  virtual_batch_size: 128
  mask_type: "sparsemax"
  loss_type: "CrossEntropyLoss"
  optimizer_lr: 0.005
  optimizer_weight_decay: 0.0
  scheduler_step_size: 50
  scheduler_gamma: 0.9

callbacks:
  model_checkpoint:
    monitor: "val/f1_weighted" # name of the logged metric which determines when model is improving
    mode: "max" # can be "max" or "min"
    save_top_k: 1 # save k best models (determined by above metric)
    save_last: False # additionaly always save model from last epoch
    verbose: False
    dirpath: "checkpoints/"
    filename: "{epoch:03d}"
    auto_insert_metric_name: False

  early_stopping:
    monitor: "val/f1_weighted" # name of the logged metric which determines when model is improving
    mode: "max" # can be "max" or "min"
    patience: 25 # how many epochs of not improving until training stops
    min_delta: 0 # minimum change in the monitored metric needed to qualify as an improvement
